{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Supervised Learning and Linear Methods.\n","## Instructions and Requirements:\n","This week workshop will have two sections:\n","\n","*   Section-1: Multiple Linear Regression with Machine Learning Approach.\n","  *   Objectives:\n","      *   Understand the steps included in building Machine Learning Models.\n","  *   Datasets:\n","      *   \"student.csv\".\n","  *   Learning Outcomes:\n","      *   Get familiar with various steps in building Machine Learning Model.\n","      *   Implement and build Multiple Linear Regression from Scratch with python and Numpy.\n","*   Section-2: Introduction to SKLEARN.\n","  *   Objectives:\n","      *   Introduction of SKLEARN, a machine learning model building library.\n","  *   Learning Outcomes:\n","      *   Ger familiar with SKLEARN library.\n","      \n","## Requirements:\n","Notebook Environment(Jupyter or Google Colab)\n","\n","*   Author: Siman Giri\n"],"metadata":{"id":"DceHorhnGtZL"}},{"cell_type":"markdown","source":["# Regression Algorithm.\n","___\n","\n","The task of the Regression Algorithm is to find the $\\color{purple}{maping}$ function black to map the $\\color{black}{input}$ variable $\\textbf{(X)}$ to the continious output variable$\\textbf{(Y)}$.\n","\n","Regression analysis estimates the realtionship between a dependent variables and independent variables.\n","In the class we talked about two different approach\n","\n","1.   Analytical Approach i.e Ordinary Least Square Methods\n","2.   Machine Learning Approach i.e Using Gradient Descent\n","In this exercise you are expected to implement both the methods from scratch.\n"],"metadata":{"id":"C2Gcxb-EMOrN"}},{"cell_type":"markdown","source":["# Section-1: Multiple Linear Regression With Machine Learning Approach.\n"],"metadata":{"id":"si4xFjkzDn6V"}},{"cell_type":"markdown","source":["##Implement Multiple Linear Regression with Gradient Descent From scratch.\n","\n","Multiple Linear Regression is a type of Linear Regression when the input has multiple features ((variables)).Similar to Simple Linear Regression, we have input variable(X) and output variable(Y). But the input variable has nn features. Therefore, we can represent this linear model as follows;\n","\\begin{align}\n","        \\mathbf{Y} = \\theta_o + \\theta_1x_1 + \\theta_2x_2 + .....+ \\theta_nx_n\n","    \\end{align}\n","Rewriting the equation:where $x_0 = 1$:\n","\\begin{align}\n","        \\mathbf{Y} = \\theta_ox_0 + \\theta_1x_1 + \\theta_2x_2 + .....+ \\theta_nx_n\n","    \\end{align}\n","Convert the equation to matrix:\n","\\begin{align}\n","        \\mathbf{Y} = \\theta^T X + \\theta_0\n","    \\end{align}\n","where:\n","\\begin{align}\n","        \\mathbf{\\theta} = [\\theta_o, \\theta_1, ...., \\theta_n]^T\n","    \\end{align}\n","\\begin{align}\n","        \\mathbf{X} = [x_o, x_1, ...., x_n]^T\n","    \\end{align}\n","\n","Our function can now be defined as:\n","\\begin{align}\n","        \\mathbf{h_\\theta(.)}  = \\theta^T x\n","    \\end{align}\n","And the cost function will be:\n","\\begin{align}\n","        \\mathbf{J(\\theta)}  = \\frac{1}{2n} \\sum_{i=1}^n(h_\\theta(x_i) - y_i)^2\n","    \\end{align}\n","\n","Implementation:\n","\n","Start with Necessary Imports:\n"],"metadata":{"id":"zeujUqB1Etrt"}},{"cell_type":"code","source":["path2 = \"/content/drive/MyDrive/5CS037-Workshops/Week-9-Workshops/student.csv\"\n"],"metadata":{"id":"oUJ5Mcr8DrPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Necessary imports:\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = (10.0, 10.0)"],"metadata":{"id":"cymxmx0SFitX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## STEP-1: Define Decision Process.\n","\n","*   Objective of the Task:\n","  *   To Predict the marks obtained in writing based on the marks of Math and Reading.\n","*   Tasks to Do:\n","  1.   Read and Observe the Dataset.\n","  2.   Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n","  3.   Print the Information of Datasets.\n","  3.   Identify $\\color{red}{\\textbf{Dependent}}$ $\\color{red}{\\textbf{Variable}}$ and $\\color{red}{\\textbf{Independent}}$ $\\color{red}{\\textbf{Variable}}$.\n","            *   {Hint: $\\textbf{Variables}$ depends on the $\\textbf{Objective}$ of the task.}\n","  1.   Split the data into Independent and Dependent Arrays { Hint: Split and Convert pandas dataframe to numpy arrays-You can use df.to_numpy()}.\n","  6. Visualize the dataset, Visualizing multi-variate dataset is not easy, In this particular case we can use special 3-D plot, Please go through the code."],"metadata":{"id":"cC4HPAgHGebF"}},{"cell_type":"code","source":[],"metadata":{"id":"mR394XnFGZSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bV_HkzhtG1r5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity Check\n","if math.shape == read.shape == write.shape:\n","  print(\"Proceed Further\")\n","else:\n","  print(\"Array Conversion Error: Try Again\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"cYDbUvGcG4EJ","outputId":"9f235187-e4e5-42a3-b314-4b1704b08ae3","executionInfo":{"status":"error","timestamp":1677757048377,"user_tz":-345,"elapsed":10,"user":{"displayName":"Sunita parajuli","userId":"16847566136961645981"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-81b8604bea07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sanity Check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Proceed Further\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Array Conversion Error: Try Again\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"]}]},{"cell_type":"markdown","source":["### Visualize the Dataset."],"metadata":{"id":"jnfXVtTho52P"}},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","# Ploting the scores as scatter plot\n","fig = plt.figure()\n","ax = plt.add_subplot(111, projection = '3d')\n","ax.scatter(math, read, write, color='#ef1234')\n","plt.show()"],"metadata":{"id":"YUhVrywkok_q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Store the arrays in the Matrix Form:\n","Implement Following Equations:\n","\n","\\begin{align}\n","        \\mathbf{Y} = \\theta^T X + \\theta_0\n","    \\end{align}\n","In Machine Learning parameters $\\theta$ are also called weights, so from this step onwards we represent $\\theta$ as $\\textbf{W}$.\n","\n","Here-Now:\n","\\begin{align}\n","        \\mathbf{Y} = W^T X + W_0\n","    \\end{align}\n","\n","\\begin{align}\n","        \\mathbf{W} = [\\theta_o, \\theta_1, ...., \\theta_n]^T\n","    \\end{align}\n","\\begin{align}\n","        \\mathbf{X} = [x_o, x_1, ...., x_n]^T\n","    \\end{align}"],"metadata":{"id":"U8I8hv_WpNlW"}},{"cell_type":"code","source":[],"metadata":{"id":"kQFXQW1lo-rT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step-2: Implement a Error/Cost Function.\n","\n","We will use the Cost function: Mean Square Error i.e.\n","\\begin{align}\n","        \\mathbf{J(\\theta)}  = \\frac{1}{2m} \\sum_{i=1}^m(y_{pred} - y_i)^2\n","    \\end{align}\n","\n","Further Simplifications:\n","\n","\\begin{align}\n","        \\mathbf{J(\\theta)}  = \\frac{1}{2m} \\sum_{i=1}^m(h_\\theta(x_i) - y_i)^2\n","    \\end{align}\n","Here:\n","\\begin{align}\n","  h_\\theta = \\theta_1*x\n","    \\end{align}\n","\n","Note:\n","In Machine Learning, m: denotes the length of the array (total No of rows in our case).\n","\n"],"metadata":{"id":"WmR-btDvtKNF"}},{"cell_type":"code","source":["#Define the cost function\n","def cost_function(X, Y, W):\n","    \"\"\" Parameters:\n","    This function finds the Mean Square Error.\n","    Input parameters:\n","      X: Feature Matrix\n","      Y: Target Matrix\n","      W: Weight Matrix\n","    Output Parameters:\n","      J: accumulated mean square error.\n","    \"\"\"\n","    # Your code here:\n","\n","\n","    return J"],"metadata":{"id":"t-X-cvAiq5xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inital_cost = cost_function(X2, Y2, W)\n","print(inital_cost)"],"metadata":{"id":"OVAFyGFnvIQV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step-3: Optimization Algorithm\n","\n","Our goal in this task is to minimize our error function with each input of examples(input data).\n","One of the most suitable methods to do the same is\n","\\begin{align}\n","$\\color{red}{\\textbf{GRADIENT DESCENT ALGOROTHIM.}}$\n","\\end{align}\n","\n","Implement GDA from Scratch.\n","\n","Function to estimate:\n","  *   $h_\\theta(x) = \\theta^T x$\n","\n","Loss function to minimize:\n","  *   $(h_\\theta(x) - y)^2$\n","\n","Gradient: Decsent update:\n","  *   $\\theta_(j+1)   = \\theta_j - LR(D_\\theta)$\n","  *   $D(\\theta) = 1/m * (\\theta^T * X)$ i.e. Derivative of parameters/weights.\n","\n","\n","\n"],"metadata":{"id":"1KKgLWzQvvyw"}},{"cell_type":"code","source":["def gradient_descent(X, Y, B, alpha, iterations):\n","    cost_history = [0] * iterations\n","    m = len(Y)\n","\n","    for iteration in range(iterations):\n","        # Hypothesis Values\n","        Y_pred = X.dot(W)\n","        # Difference b/w Hypothesis and Actual Y\n","        loss = Y-Y_pred\n","        # Gradient Calculation\n","        dw = X.T.dot(loss)/m\n","        # Changing Values of B using Gradient\n","        W_update = W - alpha *dw\n","        # New Cost Value\n","        cost = cost_function(X, Y, W_update)\n","        cost_history[iteration] = cost\n","\n","    return W_update, cost_history\n"],"metadata":{"id":"ZF1sci_UvPWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 100000 Iterations\n","alpha = 0.0001 # Learning Rate.\n","new_weights, cost_history = gradient_descent(X2, Y2, W, alpha, 100000)\n","\n","# New Values of\n","print(new_weights)\n","\n","# Final Cost of our Iterations.\n","print(cost_history[-1])"],"metadata":{"id":"bSLx3dJ00Ub-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step-4: Evaluate a Model.\n","\n","In this section, we will use $b_1$ and $b_0$ calculated from step-2 to make a prediction.\n","\n","Lets see How Good is our model. As discussed in the lecture and Tutorial we will use Root Mean Squared Error and Coefficient of Determination( R2 )\n","Mathematically:\n","\\begin{align}\n","        \\mathbf{RMSE}  = \\sqrt{\\sum_{i=1}^n (1/m (\\hat{y_i} - y_i)^2)}\n","    \\end{align}\n","\n","\\begin{align}\n","        \\mathbf{R^2}  = 1 - \\frac{SSR}{SST}\n","    \\end{align}\n","\n","\\begin{align}\n","         \\mathbf{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n","    \\end{align}\n","\\begin{align}\n","         \\mathbf{SSR} = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\n","    \\end{align}"],"metadata":{"id":"kH1Zly2y7ipu"}},{"cell_type":"code","source":["# Model Evaluation - RMSE\n","def rmse(Y, Y_pred):\n","  \"\"\"\n","  This Function calculates the Root Mean Squres.\n","  Input Arguments:\n","    Y: Array of actual(Target) Dependent Varaibles.\n","    Y_pred: Array of predeicted Dependent Varaibles.\n","  Output Arguments:\n","    rmse: Root Mean Square.\n","  \"\"\"\n","  rmse =\n","  return rmse\n","\n","# Model Evaluation - R2\n","\n","def r2(Y, Y_pred):\n","  \"\"\"\n","   This Function calculates the R Squared Error.\n","  Input Arguments:\n","    Y: Array of actual(Target) Dependent Varaibles.\n","    Y_pred: Array of predeicted Dependent Varaibles.\n","  Output Arguments:\n","    rsquared: R Squared Error.\n","    \"\"\"\n","  mean_y = np.mean(Y)\n","  ss_tot =\n","  ss_res =\n","  r2 =\n","  return r2\n",""],"metadata":{"id":"FpD9slr30r_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Let's Check Model Performance:\n"],"metadata":{"id":"8sqCKGKn2_N7"}},{"cell_type":"code","source":["Y_pred = X2.dot(new_weights)\n","\n","print(rmse(Y2, Y_pred))\n","print(r2(Y2, Y_pred))"],"metadata":{"id":"T2ZMgolG2Bq4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section-2: Introduction to SKLEARN.\n","What is scikit-learn or sklearn?\n",">  \n","\n","1.   Scikit-learn is probably the most useful library for machine learning in python.\n","2.   The sklearn library contains a lot of efficient tools for machine learning and statistical modeling\n","\n","> Please note that sklearn is used to build machine learning models. It should not be used for reading the data, manipulating and summarizing it. There are better libraries for that (e.g. NumPy, Pandas etc.)\n"],"metadata":{"id":"744MDNiG3fbP"}},{"cell_type":"markdown","source":["## Build Linear Regression with help of SKLEARN.\n"],"metadata":{"id":"7fPUvp2U46RX"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# X and Y Values\n","X = np.array([math, read]).T\n","Y = np.array(write)\n","\n","# Model Intialization\n","reg = LinearRegression()\n","# Data Fitting\n","reg = reg.fit(X, Y)\n","# Y Prediction\n","Y_pred = reg.predict(X)\n","\n","# Model Evaluation\n","rmse = np.sqrt(mean_squared_error(Y, Y_pred))\n","r2 = reg.score(X, Y)\n","\n","print(rmse)\n","print(r2)"],"metadata":{"id":"qyrN7n_W3FDR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Practise Tasks:\n","\n","For the following Datasets:\n","\n","Salary.csv: Try implementing Linear Regression with OLS and explain the Result.\n","\n","Insurance.csv: Try implementing Multiple Linear Regression from Scratch: While Doing that also practise following:\n","  1.   Can you Identify Qualitative and Quantitative Varaible.\n","  2.   Drop all the Qualitative Varaible from the dataframe.\n","  3.   If you do not want to drop the qualitative varaible, you can convert them into numeric variable as you did in Assignment-1. {You can also try sklearn label encoder function.}\n","  4.   Practise your Visualization Skills.\n","  \n","\n"],"metadata":{"id":"1MYP_WI56Ucz"}}]}